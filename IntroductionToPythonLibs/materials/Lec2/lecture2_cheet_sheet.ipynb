{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c108543e",
   "metadata": {},
   "source": [
    "# Scikit-Learn\n",
    "\n",
    "### Great for the 'fit-predict' data science\n",
    "\n",
    "\n",
    "#### You probably study that in Zolotych's class, so let's just walk through a simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b47ef385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b991f38c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.tree\n",
    "\n",
    "iris = sklearn.datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "clf = sklearn.tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X, y)\n",
    "pred = clf.predict(X)\n",
    "np.mean(pred == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e048cf4",
   "metadata": {},
   "source": [
    "# TensorFlow\n",
    "\n",
    "Let's review this example: https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/3_NeuralNetworks/autoencoder.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bb79ec",
   "metadata": {},
   "source": [
    "# Keras\n",
    "\n",
    "#### Also great for the 'fit-predict' data science\n",
    "\n",
    "\n",
    "Run this example: https://keras.io/examples/vision/mnist_convnet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36b36b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 15:31:10.271149: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-18 15:31:10.405569: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-11-18 15:31:10.439024: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-18 15:31:11.102803: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-18 15:31:11.102872: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-18 15:31:11.102881: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# Load the data and split it between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d638f878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1600)              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1600)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                16010     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34,826\n",
      "Trainable params: 34,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 15:31:12.874853: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-11-18 15:31:12.874939: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-11-18 15:31:12.876091: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e12596b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "422/422 [==============================] - 12s 28ms/step - loss: 0.3699 - accuracy: 0.8865 - val_loss: 0.0782 - val_accuracy: 0.9800\n",
      "Epoch 2/3\n",
      "422/422 [==============================] - 12s 28ms/step - loss: 0.1100 - accuracy: 0.9656 - val_loss: 0.0570 - val_accuracy: 0.9842\n",
      "Epoch 3/3\n",
      "422/422 [==============================] - 12s 28ms/step - loss: 0.0827 - accuracy: 0.9745 - val_loss: 0.0469 - val_accuracy: 0.9883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcaa538f730>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 3\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6639edc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.04786296933889389\n",
      "Test accuracy: 0.9836999773979187\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e42e449",
   "metadata": {},
   "source": [
    "# PyTorch\n",
    "\n",
    "I would recommend that to a friend! Let's review the key features in detail\n",
    "\n",
    "If want a Keras-like framework, check out PyTorch-lightning https://www.pytorchlightning.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0a1f620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a694578",
   "metadata": {},
   "source": [
    "## Build a training example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1c43286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc922b72630>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 3\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10000\n",
    "\n",
    "random_seed = 1\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4133d9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader( # will discuss the dataloaders later in detail!\n",
    "  torchvision.datasets.MNIST('./files/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, num_workers=4, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('./files/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc177ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76da9a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parent:\n",
    "    def some_function(self):\n",
    "        pass\n",
    "\n",
    "class Child(Parent):\n",
    "    def child_function():\n",
    "        pass\n",
    "child1 = Child()\n",
    "child1.some_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "376b1059",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.conv2_drop = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(1600, 10)\n",
    "\n",
    "    def print_im_fine():\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2, stride=2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2, stride=2))\n",
    "        x = x.reshape(len(x), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = x.reshape(len(x), -1)\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87bde4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Net()\n",
    "optimizer = optim.Adam(network.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "692ecff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e00705ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    network.train()\n",
    "    os.makedirs(\"./results\", exist_ok=True)\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "        loss = F.nll_loss(output, target, reduction='sum')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, \n",
    "                                                                           batch_idx * len(data),\n",
    "                                                                           len(train_loader.dataset),\n",
    "                                                                           100. * batch_idx / len(train_loader),\n",
    "                                                                           loss.item()))\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append((batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "            torch.save(network.state_dict(), './results/model.pth')\n",
    "            torch.save(optimizer.state_dict(), './results/optimizer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90d0ca60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = network(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False, reduction='sum').item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, len(test_loader.dataset), \n",
    "                                                                              100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe4677f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.2828, Accuracy: 1431/10000 (14%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                 | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 148.838547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|████████████████████████████████████████▎                                                                                | 1/3 [00:08<00:17,  8.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.1186, Accuracy: 9705/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 3.402271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|████████████████████████████████████████████████████████████████████████████████▋                                        | 2/3 [00:17<00:08,  8.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0943, Accuracy: 9728/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 5.420739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:26<00:00,  8.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0933, Accuracy: 9762/10000 (98%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test()\n",
    "for epoch in tqdm(range(1, n_epochs + 1)):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a6588e",
   "metadata": {},
   "source": [
    "### Check out the weights, gradients and stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ce7cd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.conv2_drop = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(1600, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2, stride=2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2, stride=2))\n",
    "        x = x.reshape(len(x), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = x.reshape(len(x), -1)\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ff46c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iterator = iter(test_loader)\n",
    "data, target = next(test_iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0c206d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = network.conv1.weight.data\n",
    "network.conv2.weight.data.shape\n",
    "network.conv2.weight.data[:32, :1] = w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "26f07482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 3, 3])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.conv1.weight.grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "731b5669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad tensor([0.0031, 0.0003, 0.0003])\n"
     ]
    }
   ],
   "source": [
    "torch.random.manual_seed(4)\n",
    "network = Net()\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "optimizer.zero_grad()\n",
    "output = network(data)\n",
    "loss = F.nll_loss(output, target)\n",
    "loss.backward(retain_graph=True)\n",
    "print(\"grad\", network.conv1.weight.grad[0,0,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fe55d808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad tensor([0.0063, 0.0005, 0.0007])\n"
     ]
    }
   ],
   "source": [
    "torch.random.manual_seed(4)\n",
    "# test_iterator = iter(test_loader)\n",
    "# data, target = next(test_iterator)\n",
    "network = Net()\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "optimizer.zero_grad()\n",
    "output = network(data)\n",
    "loss = F.nll_loss(output, target)\n",
    "loss.backward(retain_graph=True)\n",
    "output = network(data)\n",
    "loss = F.nll_loss(output, target)\n",
    "loss.backward(retain_graph=True)\n",
    "print(\"grad\", network.conv1.weight.grad[0,0,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0e11f289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05253171920776367"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "with torch.no_grad():\n",
    "    output = network(data)\n",
    "time.time() - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cd182070",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = network.half().cuda()\n",
    "data = data.half().cuda()\n",
    "with torch.cuda.amp.autocast():\n",
    "    with torch.no_grad():\n",
    "        output = network(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc26484f",
   "metadata": {},
   "source": [
    "## Check out the dataloader conception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0b596078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_monkey_job_in_one_thread(n=1e6, N=300):\n",
    "    x = np.zeros(N)\n",
    "    for _ in range(int(n)):\n",
    "        x+=1\n",
    "        x-=1\n",
    "do_monkey_job_in_one_thread()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f5d5ff3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c7acba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    print(\"Do stuff with the batch from process id=\", multiprocessing.current_process().ident)\n",
    "\n",
    "    do_monkey_job_in_one_thread()\n",
    "    return batch[0], batch[1]\n",
    "\n",
    "class AwesomeParallelDataLoader(torch.utils.data.Dataset):\n",
    "    def __init__(self, val_set=True):\n",
    "        print(\"AwesomeParallelDataLoader init ok\")\n",
    "        self.length = 1000\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, dataset_idx):\n",
    "        np.random.seed(multiprocessing.current_process().ident)\n",
    "        time.sleep(np.random.rand())\n",
    "        print(\"call __getitem__ with idx=\", dataset_idx, \"from process id=\", multiprocessing.current_process().ident)\n",
    "        do_monkey_job_in_one_thread()\n",
    "        return np.zeros((1,28,28), dtype=np.uint8), 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bcc7929d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AwesomeParallelDataLoader init ok\n",
      "call __getitem__ with idx= 0 from process id= 1587564\n",
      "call __getitem__ with idx= 3 from process id= 1587596\n",
      "call __getitem__ with idx= 1 from process id= 1587564\n",
      "call __getitem__ with idx= 4 from process id= 1587596\n",
      "call __getitem__ with idx= 2 from process id= 1587564\n",
      "call __getitem__ with idx= 5 from process id= 1587596\n",
      "Do stuff with the batch from process id= 1587564\n",
      "Do stuff with the batch from process id= 1587596\n",
      "call __getitem__ with idx= 6 from process id= 1587564\n",
      "call __getitem__ with idx= 7 from process id= 1587564\n",
      "call __getitem__ with idx= 8 from process id= 1587564\n",
      "Do stuff with the batch from process id= 1587564\n"
     ]
    }
   ],
   "source": [
    "test_dataset = AwesomeParallelDataLoader(val_set=True)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=3, \n",
    "                                           shuffle=False, num_workers=2,\n",
    "                                           pin_memory=True, collate_fn=collate_fn, prefetch_factor=1)\n",
    "batch_iterator = iter(test_data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "449fbd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target = next(batch_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd4136d",
   "metadata": {},
   "source": [
    "# Convert the model to onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "724830a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3e688097",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1556555/536731236.py:12: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  x = x.reshape(len(x), -1)\n",
      "/tmp/ipykernel_1556555/536731236.py:14: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  x = x.reshape(len(x), -1)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 1, 28, 28, requires_grad=True).float()\n",
    "network = network.cpu().float()\n",
    "torch_out = network(x)\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(network,               # model being run\n",
    "                  x,                         # model input (or a tuple for multiple inputs)\n",
    "                  \"some_network.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=10,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['input'],   # the model's input names\n",
    "                  output_names = ['output'], # the model's output names\n",
    "                  dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
    "                                'output' : {0 : 'batch_size'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11bfc40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
