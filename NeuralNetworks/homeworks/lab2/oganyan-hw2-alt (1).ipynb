{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"a55375f1da33de460d61f26d01bd36bfbcd8e12064599fa5a58833b6ccc82f6a"}},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Оганян Роберт. Выполнено с помощью Kaggle","metadata":{}},{"cell_type":"markdown","source":"# Transformers\n\nIn this homework, you need to generate text with a neural network.","metadata":{"id":"Igxuy75W5V6j"}},{"cell_type":"code","source":"import torch\nfrom torch import nn\n\nimport numpy as np\nimport time\n\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport math\nimport random","metadata":{"id":"N8_Ha3x-5V6m","execution":{"iopub.status.busy":"2022-12-24T09:28:58.302166Z","iopub.execute_input":"2022-12-24T09:28:58.303447Z","iopub.status.idle":"2022-12-24T09:28:58.821318Z","shell.execute_reply.started":"2022-12-24T09:28:58.303365Z","shell.execute_reply":"2022-12-24T09:28:58.820354Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Task 1 (0.5 points)\nWe will train a language model to predict the next letter. Such language models are used in speech recognition, as they provide additional information to the acoustic model when the next character is selected. To get started, open the data, check what characters are included in the texts, how many there are. Remove all newline characters and tabs from the text.","metadata":{"id":"lFXsDKRZ5V6n"}},{"cell_type":"code","source":"path = '/kaggle/input/smallcorp/small_corp_for_test.txt'\nfile = open(path, 'r')\ndata = file.readlines()\nfile.close()\nlen(data)","metadata":{"scrolled":true,"id":"1M_I40Zl5V6n","execution":{"iopub.status.busy":"2022-12-24T09:28:58.827805Z","iopub.execute_input":"2022-12-24T09:28:58.828471Z","iopub.status.idle":"2022-12-24T09:28:59.128610Z","shell.execute_reply.started":"2022-12-24T09:28:58.828433Z","shell.execute_reply":"2022-12-24T09:28:59.127542Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"700000"},"metadata":{}}]},{"cell_type":"code","source":"data[:5]","metadata":{"execution":{"iopub.status.busy":"2022-12-24T09:28:59.130177Z","iopub.execute_input":"2022-12-24T09:28:59.130595Z","iopub.status.idle":"2022-12-24T09:28:59.139510Z","shell.execute_reply.started":"2022-12-24T09:28:59.130556Z","shell.execute_reply":"2022-12-24T09:28:59.138298Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"['добро\\n', 'кого\\n', 'капитан\\n', 'нет\\n', 'зачем\\n']"},"metadata":{}}]},{"cell_type":"code","source":"# YOUR CODE HERE\ndata = [s.rstrip('\\n') for s in data]\ndata = [s.rstrip('\\t') for s in data]","metadata":{"id":"MDpR5MxM5V6o","execution":{"iopub.status.busy":"2022-12-24T09:28:59.143617Z","iopub.execute_input":"2022-12-24T09:28:59.144013Z","iopub.status.idle":"2022-12-24T09:28:59.438770Z","shell.execute_reply.started":"2022-12-24T09:28:59.143987Z","shell.execute_reply":"2022-12-24T09:28:59.437767Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"data[:5]","metadata":{"execution":{"iopub.status.busy":"2022-12-24T09:28:59.440349Z","iopub.execute_input":"2022-12-24T09:28:59.440741Z","iopub.status.idle":"2022-12-24T09:28:59.464512Z","shell.execute_reply.started":"2022-12-24T09:28:59.440702Z","shell.execute_reply":"2022-12-24T09:28:59.463342Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"['добро', 'кого', 'капитан', 'нет', 'зачем']"},"metadata":{}}]},{"cell_type":"markdown","source":"## Task 2 (0.5 points)\nTo train the model, you have to change the text to a form suitable for the neural network. It is also important to note that we need to add two tokens (start and end), which are responsible for the beginning and end of the text. Use [ and ] for this task. We also need a pad token to fill the text with it to the required length to form a batch.\n\nImplement the preprocess method of the Preprocessor class. As input it takes text and the length of text that we expect to receive as output. The text must be converted to lower case, the required number of pad tokens are added to the end of the text, then the text is vectorized (each character has its own number). You need to return two vectors: the result obtained without the last token (we will train on it) and the result obtained without the first token (target during training).","metadata":{"id":"rlGTWEMK5V6o"}},{"cell_type":"markdown","source":"Видимо **_** это символ паддинга. Также здесь нет буквы \"Ф\" (добавил сам)","metadata":{}},{"cell_type":"code","source":"class Preprocessor:\n    def __init__(self):\n        self.alphabet = '_добсркгаупитнезчм яжлйвцыэь-шхющёъф][ '\n        self.token2ind = {}\n        self.ind2token = {}\n        for i in range(len(self.alphabet)):\n            self.token2ind[self.alphabet[i]] = i\n            self.ind2token[i] = self.alphabet[i]\n        \n    \n    def preprocess(self, text, window_size):\n        # YOUR CODE HERE\n        text = text.lower()\n        pad_cnt = window_size - len(text)\n        text += '_' * pad_cnt\n        vector = [self.token2ind[i] for i in text]\n        return vector[:-1], vector[1:]","metadata":{"id":"MCu24Nez5V6o","execution":{"iopub.status.busy":"2022-12-24T09:28:59.466018Z","iopub.execute_input":"2022-12-24T09:28:59.466968Z","iopub.status.idle":"2022-12-24T09:28:59.476077Z","shell.execute_reply.started":"2022-12-24T09:28:59.466925Z","shell.execute_reply":"2022-12-24T09:28:59.475023Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"txt1 = 'Всем привет'\nprint(txt1, len(txt1))\nPreprocessor().preprocess(txt1, len(txt1) + 5)","metadata":{"execution":{"iopub.status.busy":"2022-12-24T09:28:59.477482Z","iopub.execute_input":"2022-12-24T09:28:59.478498Z","iopub.status.idle":"2022-12-24T09:28:59.489306Z","shell.execute_reply.started":"2022-12-24T09:28:59.478462Z","shell.execute_reply":"2022-12-24T09:28:59.488143Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Всем привет 11\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"([23, 4, 14, 17, 38, 10, 5, 11, 23, 14, 12, 0, 0, 0, 0],\n [4, 14, 17, 38, 10, 5, 11, 23, 14, 12, 0, 0, 0, 0, 0])"},"metadata":{}}]},{"cell_type":"markdown","source":"## Task 3 (0.5 points)\nSince we decided that the text will begin with the token [ and end with the token ], the data needs to be corrected. Implement this idea, add these tokens to your texts.","metadata":{"id":"VRqJTtxC5V6p"}},{"cell_type":"code","source":"# YOUR CODE HERE\ndata = ['[' + word + ']' for word in data]\ndata[:5]","metadata":{"id":"417DsdjZ5V6p","execution":{"iopub.status.busy":"2022-12-24T09:28:59.491169Z","iopub.execute_input":"2022-12-24T09:28:59.491606Z","iopub.status.idle":"2022-12-24T09:28:59.713328Z","shell.execute_reply.started":"2022-12-24T09:28:59.491573Z","shell.execute_reply":"2022-12-24T09:28:59.712267Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"['[добро]', '[кого]', '[капитан]', '[нет]', '[зачем]']"},"metadata":{}}]},{"cell_type":"markdown","source":"## Task 4 (0.5 points)\nLet's limit the maximum text length. You can change this threshold and thereby reduce the number of texts in your sample and increase the learning rate. Let's start with 128.\nSelect a threshold and leave only those texts whose length does not exceed this threshold.\n\nNext, split the texts into train and test, mix the texts when splitting, the size of the test sample should be 15% of the total number of texts.","metadata":{"id":"tl_QKCTW5V6p"}},{"cell_type":"code","source":"THRESHOLD = 128\n\n# YOUR CODE HERE\n\ndata = [text for text in data if len(text) <= THRESHOLD]\nprint(f'data_size = {len(data)}')","metadata":{"id":"ue53yUA65V6q","execution":{"iopub.status.busy":"2022-12-24T09:28:59.714950Z","iopub.execute_input":"2022-12-24T09:28:59.715381Z","iopub.status.idle":"2022-12-24T09:28:59.821354Z","shell.execute_reply.started":"2022-12-24T09:28:59.715343Z","shell.execute_reply":"2022-12-24T09:28:59.820396Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"data_size = 683438\n","output_type":"stream"}]},{"cell_type":"code","source":"SIZE_FRAC = 0.85\ntrain_size = int(SIZE_FRAC * (len(data)))\ntest_size = len(data) - train_size\nprint(f'train_size = {train_size},', f'test_size = {test_size}')\n\ndata_train, data_test = random_split(data, [train_size, test_size])","metadata":{"execution":{"iopub.status.busy":"2022-12-24T09:28:59.822845Z","iopub.execute_input":"2022-12-24T09:28:59.823205Z","iopub.status.idle":"2022-12-24T09:28:59.882159Z","shell.execute_reply.started":"2022-12-24T09:28:59.823171Z","shell.execute_reply":"2022-12-24T09:28:59.881198Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"train_size = 580922, test_size = 102516\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Task 5 (1.5 points)\nLet's write a dataset. The input to the dataset is a set of texts, an object of the Preprocessor class, and the window size that you selected in the previous task.\nImplement the __len__ and __getitem__ methods.","metadata":{"id":"ZRRp7W1j5V6q"}},{"cell_type":"code","source":"class TextDataset(torch.utils.data.Dataset):\n    \n    def __init__(self, x, preproc, win_size = 128):\n        # YOUR CODE HERE\n        self.data = [preproc.preprocess(text, win_size) for text in x]\n    \n    def __len__(self):\n        # YOUR CODE HERE\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        # YOUR CODE HERE\n        return torch.tensor(self.data[idx][0], dtype=torch.int64),\\\n    torch.tensor(self.data[idx][1], dtype=torch.int64)","metadata":{"id":"tCA9-2NP5V6q","execution":{"iopub.status.busy":"2022-12-24T09:28:59.883650Z","iopub.execute_input":"2022-12-24T09:28:59.884303Z","iopub.status.idle":"2022-12-24T09:28:59.914490Z","shell.execute_reply.started":"2022-12-24T09:28:59.884265Z","shell.execute_reply":"2022-12-24T09:28:59.913291Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"preproc = Preprocessor()\ntrain_dataset = TextDataset(data_train, preproc)\ntest_dataset = TextDataset(data_test, preproc)","metadata":{"id":"cZhUrshc5V6q","execution":{"iopub.status.busy":"2022-12-24T09:28:59.915872Z","iopub.execute_input":"2022-12-24T09:28:59.916842Z","iopub.status.idle":"2022-12-24T09:29:18.940939Z","shell.execute_reply.started":"2022-12-24T09:28:59.916804Z","shell.execute_reply":"2022-12-24T09:29:18.939952Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"\n\n## Task 6 (1.5 points)\nLet's write a model. The class for implementing positional encoding is implemented for you, it is needed so that the model (after receiving embeddings) can understand, in which place which token is located.\n\nFill in the blanks in the model class. Choose the hyperparameters of the model. It is recommended to use no more than 6 layers in the transformer. In the decoder, use two linear layers with a ReLU activation function in between.","metadata":{"id":"G4GIeH3n5V6q"}},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)","metadata":{"id":"BIbyJo_u5V6r","execution":{"iopub.status.busy":"2022-12-24T09:29:18.945734Z","iopub.execute_input":"2022-12-24T09:29:18.946638Z","iopub.status.idle":"2022-12-24T09:29:18.955221Z","shell.execute_reply.started":"2022-12-24T09:29:18.946600Z","shell.execute_reply":"2022-12-24T09:29:18.954139Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class LanguageModel(nn.Module):\n    def __init__(self, vocab_size, embedding_size, n_layers, n_heads,\n                 dim_feedforward, dropout, max_len=5000):\n        super(LanguageModel, self).__init__()\n        self.emb = nn.Embedding(vocab_size, embedding_size)\n        self.pe = PositionalEncoding(embedding_size, dropout, max_len)\n        self.transformer_encoder_layer = nn.\\\n        TransformerEncoderLayer(embedding_size,n_heads, dim_feedforward, dropout)\n        self.transformer_encoder = nn.TransformerEncoder(\n            self.transformer_encoder_layer, n_layers)\n        self.decoder = nn.Sequential(\n            nn.Linear(embedding_size, embedding_size),\n            nn.ReLU(),\n            nn.Linear(embedding_size, vocab_size)\n        )\n    \n    def forward(self, x, src_mask):\n        x = self.pe(self.emb(x)) # emb, then pe\n        x = x.transpose(1, 0)\n        x = self.transformer_encoder(x , src_mask) # transformer encoder with mask\n        x = self.decoder(x) # decoder\n        return x.transpose(1, 0)\n    \n    def generate_square_subsequent_mask(self, sz):\n        \n        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n        return mask","metadata":{"id":"80WG1r7O5V6r","execution":{"iopub.status.busy":"2022-12-24T09:29:18.956401Z","iopub.execute_input":"2022-12-24T09:29:18.956663Z","iopub.status.idle":"2022-12-24T09:29:18.970392Z","shell.execute_reply.started":"2022-12-24T09:29:18.956639Z","shell.execute_reply":"2022-12-24T09:29:18.969365Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"I've tried some hyperparameters combinations (did not include that in notebook) and these are optimal:","metadata":{}},{"cell_type":"code","source":"vocab_size = len('_добсркгаупитнезчм яжлйвцыэь-шхющёъф][ ')\nembedding_size = 256\nn_layers = 10\nn_heads = 4 \ndim_feedforward = 1024\ndropout = 0.2","metadata":{"execution":{"iopub.status.busy":"2022-12-24T09:29:18.982984Z","iopub.execute_input":"2022-12-24T09:29:18.983776Z","iopub.status.idle":"2022-12-24T09:29:18.991654Z","shell.execute_reply.started":"2022-12-24T09:29:18.983749Z","shell.execute_reply":"2022-12-24T09:29:18.990631Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"model = LanguageModel(vocab_size, embedding_size, n_layers,\n                     n_heads, dim_feedforward, dropout)","metadata":{"id":"qlTy35Pu5V6r","execution":{"iopub.status.busy":"2022-12-24T09:29:18.993075Z","iopub.execute_input":"2022-12-24T09:29:18.993813Z","iopub.status.idle":"2022-12-24T09:29:19.048532Z","shell.execute_reply.started":"2022-12-24T09:29:18.993779Z","shell.execute_reply":"2022-12-24T09:29:19.047590Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## Task 7 (2 points)\nLet's implement a class to train the model and validate it. Follow the directions in the code and fill in the missing pieces in the code.","metadata":{"id":"kYOtJnqB5V6r"}},{"cell_type":"code","source":"class Trainer:\n    \n    def __init__(self, model, train_dataset, test_dataset, win_size=127):\n        \n        self.model = model\n        \n        self.train_batch_size = 64\n        self.test_batch_size = 64\n        \n        self.train_dataloader = DataLoader(\n            dataset=train_dataset,\n            batch_size=self.train_batch_size,\n            num_workers=2,\n            shuffle=True,\n        )\n        \n        self.test_dataloader = DataLoader(\n            dataset=test_dataset,\n            batch_size=self.test_batch_size,\n            num_workers=2,\n            shuffle=False,\n        )\n        self.train_dataloader_size = len(self.train_dataloader)\n        self.test_dataloader_size = len(self.test_dataloader)\n        \n        self.device = 'cuda:0'        \n        self.criterion = nn.CrossEntropyLoss(ignore_index = 0) # use CrossEntrophyLoss, pass as a parameter\n                             # ignore the index of the _ character so that the model is not penalized for the character after the closing token\n\n        \n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-4, weight_decay=5e-4)\n        \n        self.steps_to_print = 1000\n        \n        self.mask = self.model.generate_square_subsequent_mask(win_size).to(self.device)\n        \n    def train_one_epoch(self, epoch_number):\n        step = 0\n        counted_loss = 0\n        current_time = time.time()\n        it = 0\n        \n        for batch in self.train_dataloader:\n            x, y = batch\n            # YOUR CODE HERE\n            # implement model training steps\n            # store the loss value in counted_loss variable\n            \n            self.optimizer.zero_grad()\n            x, y = x.to(self.device),y.to(self.device)\n            it += len(x)\n            step += 1\n            \n            pred = self.model(x,self.mask).transpose(-1,1)\n            \n            loss =  self.criterion(pred, y)            \n            counted_loss += loss.item()          \n            \n            loss.backward()\n            self.optimizer.step()            \n            \n            if step%self.steps_to_print == 0:\n                result = 'Train epoch '+str(epoch_number)+' | '\n                result += 'Step '+str(step)+'/'+str(self.train_dataloader_size)+' | '\n                result += 'Counted loss '+str(counted_loss)+' | '\n                result += 'ppl '+str(math.exp(counted_loss/it))+' | '\n                result += 'time '+str(time.time() - current_time) + ' | '\n                print(result)\n                current_time = time.time()\n                counted_loss = 0\n                it = 0\n    \n    def validate_one_epoch(self, epoch_number):\n        step = 0\n        counted_loss = 0\n        current_time = time.time()\n        it = 0\n        for batch in self.test_dataloader:\n            x, y = batch\n            \n            # YOUR CODE HERE\n            # implement steps for test\n            # remember that this method is already run from the block with torch.no_grad(), so you don't need to reuse it\n            \n            x, y = x.to(self.device),y.to(self.device)\n            it += len(x)\n            step += 1\n            \n            pred = self.model(x,self.mask).transpose(-1,1)\n            \n            loss =  self.criterion(pred, y)            \n            counted_loss+=loss.item()          \n            \n            if step%(self.steps_to_print//2) == 0:\n                result = 'Validate epoch '+str(epoch_number)+' | '\n                result += 'Step '+str(step)+'/'+str(self.test_dataloader_size)+' | '\n                result += 'Counted loss '+str(counted_loss)+' | '\n                result += 'ppl '+str(math.exp(counted_loss/it))+' | '\n                result += 'time '+str(time.time() - current_time) + ' | '\n                print(result)\n                current_time = time.time()\n                counted_loss = 0\n                it = 0\n        \n    def train(self, number_of_epochs):\n        model.to(self.device)\n        for epoch in range(1, number_of_epochs+1):\n            model.train()\n            self.train_one_epoch(epoch)\n            with torch.no_grad():\n                model.eval()\n                self.validate_one_epoch(epoch)\n            print()","metadata":{"id":"0fXoWIt75V6r","execution":{"iopub.status.busy":"2022-12-24T09:29:19.050093Z","iopub.execute_input":"2022-12-24T09:29:19.050631Z","iopub.status.idle":"2022-12-24T09:29:19.068946Z","shell.execute_reply.started":"2022-12-24T09:29:19.050594Z","shell.execute_reply":"2022-12-24T09:29:19.067791Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Task 8 (0.5 points)\nRun training on multiple epochs. Focus on your computing power and work time. You can always calculate how many seconds it takes for one batch.","metadata":{"id":"ZGt0oTKX5V6s"}},{"cell_type":"code","source":"# YOUR CODE HERE\nvocab_size = len('_добсркгаупитнезчм яжлйвцыэь-шхющёъф][ ')\nmodel = LanguageModel(vocab_size, embedding_size, n_layers,\n                     n_heads, dim_feedforward, dropout)\n\ntrainer = Trainer(model, train_dataset, test_dataset)","metadata":{"id":"GzbpIeNi5V6s","execution":{"iopub.status.busy":"2022-12-24T09:29:19.070282Z","iopub.execute_input":"2022-12-24T09:29:19.070971Z","iopub.status.idle":"2022-12-24T09:29:20.674487Z","shell.execute_reply.started":"2022-12-24T09:29:19.070935Z","shell.execute_reply":"2022-12-24T09:29:20.673480Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"trainer.train(3)","metadata":{"execution":{"iopub.status.busy":"2022-12-24T09:29:20.675901Z","iopub.execute_input":"2022-12-24T09:29:20.676269Z","iopub.status.idle":"2022-12-24T10:24:57.010724Z","shell.execute_reply.started":"2022-12-24T09:29:20.676218Z","shell.execute_reply":"2022-12-24T10:24:57.009418Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Train epoch 1 | Step 1000/9077 | Counted loss 2521.3678319454193 | ppl 1.0401827016358443 | time 116.86548614501953 | \nTrain epoch 1 | Step 2000/9077 | Counted loss 2079.3366354703903 | ppl 1.0330231857300487 | time 116.12752366065979 | \nTrain epoch 1 | Step 3000/9077 | Counted loss 1908.1332131624222 | ppl 1.0302634862862632 | time 115.90733766555786 | \nTrain epoch 1 | Step 4000/9077 | Counted loss 1820.5875327587128 | ppl 1.0288551510204347 | time 116.19647192955017 | \nTrain epoch 1 | Step 5000/9077 | Counted loss 1768.3077867031097 | ppl 1.0280150522242202 | time 116.0899178981781 | \nTrain epoch 1 | Step 6000/9077 | Counted loss 1735.173864722252 | ppl 1.0274829685560083 | time 116.14609956741333 | \nTrain epoch 1 | Step 7000/9077 | Counted loss 1704.6776942014694 | ppl 1.0269934868128179 | time 116.52597856521606 | \nTrain epoch 1 | Step 8000/9077 | Counted loss 1683.4128292798996 | ppl 1.026652311030895 | time 116.49167084693909 | \nTrain epoch 1 | Step 9000/9077 | Counted loss 1667.4205747842789 | ppl 1.0263958042513597 | time 116.40280628204346 | \nValidate epoch 1 | Step 500/1602 | Counted loss 777.9576040506363 | ppl 1.0246091011560874 | time 17.062163591384888 | \nValidate epoch 1 | Step 1000/1602 | Counted loss 780.4162398576736 | ppl 1.0246878273249176 | time 17.212401151657104 | \nValidate epoch 1 | Step 1500/1602 | Counted loss 775.6138926744461 | ppl 1.0245340605289415 | time 16.99791979789734 | \n\nTrain epoch 2 | Step 1000/9077 | Counted loss 1649.3436172008514 | ppl 1.026105937167978 | time 117.36011838912964 | \nTrain epoch 2 | Step 2000/9077 | Counted loss 1641.5942922830582 | ppl 1.0259817004973169 | time 116.3362033367157 | \nTrain epoch 2 | Step 3000/9077 | Counted loss 1636.7408822774887 | ppl 1.0259038986059714 | time 116.26211547851562 | \nTrain epoch 2 | Step 4000/9077 | Counted loss 1622.9755526781082 | ppl 1.0256832675635081 | time 116.26710271835327 | \nTrain epoch 2 | Step 5000/9077 | Counted loss 1615.197569489479 | ppl 1.0255586228375728 | time 116.34355401992798 | \nTrain epoch 2 | Step 6000/9077 | Counted loss 1611.000405550003 | ppl 1.025491368204312 | time 116.59631085395813 | \nTrain epoch 2 | Step 7000/9077 | Counted loss 1599.3948421478271 | ppl 1.0253054256093874 | time 116.4314558506012 | \nTrain epoch 2 | Step 8000/9077 | Counted loss 1594.1317002773285 | ppl 1.0252211114526084 | time 116.59424567222595 | \nTrain epoch 2 | Step 9000/9077 | Counted loss 1586.9667632579803 | ppl 1.0251063421787103 | time 116.5982871055603 | \nValidate epoch 2 | Step 500/1602 | Counted loss 740.1116470098495 | ppl 1.023398026457083 | time 17.263501167297363 | \nValidate epoch 2 | Step 1000/1602 | Counted loss 742.5008177757263 | ppl 1.0234744378297531 | time 17.049341678619385 | \nValidate epoch 2 | Step 1500/1602 | Counted loss 737.6415683031082 | ppl 1.0233190334535534 | time 16.962729454040527 | \n\nTrain epoch 3 | Step 1000/9077 | Counted loss 1579.734126329422 | ppl 1.0249905015058747 | time 116.88317155838013 | \nTrain epoch 3 | Step 2000/9077 | Counted loss 1573.8486697673798 | ppl 1.0248962474479872 | time 116.63332486152649 | \nTrain epoch 3 | Step 3000/9077 | Counted loss 1570.3682087659836 | ppl 1.024840512847553 | time 116.87439036369324 | \nTrain epoch 3 | Step 4000/9077 | Counted loss 1566.2835841178894 | ppl 1.024775107296941 | time 116.47639656066895 | \nTrain epoch 3 | Step 5000/9077 | Counted loss 1561.572387456894 | ppl 1.0246996738692735 | time 116.44771122932434 | \nTrain epoch 3 | Step 6000/9077 | Counted loss 1553.72363114357 | ppl 1.0245740156678023 | time 116.43205738067627 | \nTrain epoch 3 | Step 7000/9077 | Counted loss 1556.3163031339645 | ppl 1.024615522514036 | time 116.3444516658783 | \nTrain epoch 3 | Step 8000/9077 | Counted loss 1547.5718430280685 | ppl 1.0244755366156608 | time 116.50247979164124 | \nTrain epoch 3 | Step 9000/9077 | Counted loss 1542.6830703020096 | ppl 1.0243972829160168 | time 116.40276002883911 | \nValidate epoch 3 | Step 500/1602 | Counted loss 716.0208921432495 | ppl 1.0226278654276437 | time 17.12608242034912 | \nValidate epoch 3 | Step 1000/1602 | Counted loss 718.3378512859344 | ppl 1.0227019114514604 | time 16.994736671447754 | \nValidate epoch 3 | Step 1500/1602 | Counted loss 713.2636082172394 | ppl 1.0225397543682597 | time 17.01282501220703 | \n\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(model.state_dict(), './model')","metadata":{"execution":{"iopub.status.busy":"2022-12-24T10:24:57.014790Z","iopub.execute_input":"2022-12-24T10:24:57.015147Z","iopub.status.idle":"2022-12-24T10:24:57.114716Z","shell.execute_reply.started":"2022-12-24T10:24:57.015113Z","shell.execute_reply":"2022-12-24T10:24:57.113540Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## Task 9 (1 point)\nLet's try to generate text with our model. Finish the text generation function. Try to generate some text. Remember that if you want to generate text from scratch, you must pass only the start token as text.\nStop generating text if the model gives you an end token or if the text length is greater than 150.","metadata":{"id":"-XU8w0PP5V6s"}},{"cell_type":"code","source":"model.eval()\n''","metadata":{"execution":{"iopub.status.busy":"2022-12-24T10:25:26.297955Z","iopub.execute_input":"2022-12-24T10:25:26.298331Z","iopub.status.idle":"2022-12-24T10:25:26.305355Z","shell.execute_reply.started":"2022-12-24T10:25:26.298300Z","shell.execute_reply":"2022-12-24T10:25:26.304304Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"''"},"metadata":{}}]},{"cell_type":"code","source":"def generate_text(text):\n    x = []\n    \n    for letter in text:\n        x.append(preproc.token2ind[letter])\n    x = torch.Tensor([x]).int().to('cuda:0')\n    win_size = len(x[0])\n    \n    mask = model.generate_square_subsequent_mask(win_size).to('cuda:0')\n    pred = model(x, mask)\n    ind = torch.argmax(pred[0][-1])\n    \n    text += preproc.ind2token[ind.item()]\n    \n    if preproc.ind2token[ind.item()] == ']' or len(text) >= 150:\n        return text\n    else:\n        return generate_text(text)","metadata":{"execution":{"iopub.status.busy":"2022-12-24T10:25:33.559395Z","iopub.execute_input":"2022-12-24T10:25:33.559765Z","iopub.status.idle":"2022-12-24T10:25:33.566674Z","shell.execute_reply.started":"2022-12-24T10:25:33.559732Z","shell.execute_reply":"2022-12-24T10:25:33.565397Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"generate_text('[')","metadata":{"execution":{"iopub.status.busy":"2022-12-24T11:25:54.802675Z","iopub.execute_input":"2022-12-24T11:25:54.803462Z","iopub.status.idle":"2022-12-24T11:25:55.668492Z","shell.execute_reply.started":"2022-12-24T11:25:54.803421Z","shell.execute_reply":"2022-12-24T11:25:55.667310Z"},"trusted":true},"execution_count":187,"outputs":[{"execution_count":187,"output_type":"execute_result","data":{"text/plain":"'[а вот на подавал на сайт на сайт на сайте в подождите пожалуйста на сайта на сайта на сайте в подолжение по по поводу вам подально в подолного стовой'"},"metadata":{}}]},{"cell_type":"code","source":"generate_text('[комп')","metadata":{"execution":{"iopub.status.busy":"2022-12-24T11:26:17.725693Z","iopub.execute_input":"2022-12-24T11:26:17.726247Z","iopub.status.idle":"2022-12-24T11:26:18.456218Z","shell.execute_reply.started":"2022-12-24T11:26:17.726194Z","shell.execute_reply":"2022-12-24T11:26:18.455264Z"},"trusted":true},"execution_count":190,"outputs":[{"execution_count":190,"output_type":"execute_result","data":{"text/plain":"'[компания ашманов и партнёры секретарь андрей день меня зовут андрей день меня зовут информацию и по слушаю вас]'"},"metadata":{}}]},{"cell_type":"markdown","source":"Well, we got some results. It's not bad but not good either. The first problem is that we choose always the maximal logit (so we generate same words (letters)). Secondly, if we do sampling and randomly choose word considering probabilities, we'll get new problem: this can sometimes generate nonsense words due to the fact that softmax probabilities of these words are never exactly zero. This issue can be somewhat mitigated with sampling temperature, but low temperature harms sampling diversity. Can we remove the nonsense words without sacrificing diversity? Yes, we can! But it takes a different sampling strategy","metadata":{}},{"cell_type":"markdown","source":"__Top-k sampling:__ on each step, sample the next token from __k most likely__ candidates from the language model.\n\nSuppose $k=3$ and the token probabilities are $p=[0.1, 0.35, 0.05, 0.2, 0.3]$. You first need to select $k$ most likely words and set the probability of the rest to zero: $\\hat p=[0.0, 0.35, 0.0, 0.2, 0.3]$ and re-normalize: \n$p^*\\approx[0.0, 0.412, 0.0, 0.235, 0.353]$.\n","metadata":{}},{"cell_type":"markdown","source":"__Nucleus sampling:__ similar to top-k sampling, but this time we select $k$ dynamically. In nucleous sampling, we sample from top-__N%__ fraction of the probability mass.\n\nUsing the same  $p=[0.1, 0.35, 0.05, 0.2, 0.3]$ and nucleous N=0.9, the nucleous words consist of:\n1. most likely token $w_2$, because $p(w_2) < N$\n2. second most likely token $w_5$, $p(w_2) + p(w_5) = 0.65 < N$\n3. third most likely token $w_4$ because $p(w_2) + p(w_5) + p(w_4) = 0.85 < N$\n\nAnd thats it, because the next most likely word would overflow: $p(w_2) + p(w_5) + p(w_4) + p(w_1) = 0.95 > N$.\n\nAfter you've selected the nucleous words, you need to re-normalize them as in top-k sampling and generate the next token.\n\nLet's implement __Nucleus sampling__","metadata":{}},{"cell_type":"code","source":"NUCLEUS_CONST = 0.8\ndef generate_text_nucleus(text):\n    x = []\n    \n    for letter in text:\n        x.append(preproc.token2ind[letter])\n        \n    x = torch.Tensor([x]).int().to('cuda:0')\n    win_size = len(x[0])\n    \n    pred = model(x, model.generate_square_subsequent_mask(win_size).to('cuda:0'))\n    \n    probs = np.array(nn.Softmax()(pred[0][-1]).detach().cpu().numpy())\n    tokens = np.array(range(len(probs)))\n\n    sorted_idxes = np.argsort(probs)[::-1]\n    probs = probs[sorted_idxes]\n    tokens = tokens[sorted_idxes]\n    mask_of_taken = np.cumsum(probs) < NUCLEUS_CONST\n    \n    new_probs = probs[mask_of_taken]\n    new_tokens = tokens[mask_of_taken]\n\n    if new_probs.sum():\n        prob_factor = 1.0 / new_probs.sum()\n        new_probs *= prob_factor\n        next_token = np.random.choice(new_tokens, p=new_probs)\n    else:\n        next_token = np.random.choice(tokens, p=probs)\n    text += preproc.ind2token[int(next_token)]\n    \n    if preproc.ind2token[next_token.item()] == ']' or len(text) >= 150:\n        return text\n    else:\n        return generate_text_nucleus(text)","metadata":{"execution":{"iopub.status.busy":"2022-12-24T11:01:48.648727Z","iopub.execute_input":"2022-12-24T11:01:48.649470Z","iopub.status.idle":"2022-12-24T11:01:48.658940Z","shell.execute_reply.started":"2022-12-24T11:01:48.649432Z","shell.execute_reply":"2022-12-24T11:01:48.657994Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"generate_text_nucleus('[')","metadata":{"execution":{"iopub.status.busy":"2022-12-24T11:02:08.429502Z","iopub.execute_input":"2022-12-24T11:02:08.429865Z","iopub.status.idle":"2022-12-24T11:02:08.766052Z","shell.execute_reply.started":"2022-12-24T11:02:08.429834Z","shell.execute_reply":"2022-12-24T11:02:08.765168Z"},"trusted":true},"execution_count":100,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  del sys.path[0]\n","output_type":"stream"},{"execution_count":100,"output_type":"execute_result","data":{"text/plain":"'[так и уже как он выбираете подскажите пожалуйста не могу]'"},"metadata":{}}]},{"cell_type":"code","source":"generate_text_nucleus('[хорошая модель должна ')","metadata":{"execution":{"iopub.status.busy":"2022-12-24T11:03:17.250897Z","iopub.execute_input":"2022-12-24T11:03:17.251277Z","iopub.status.idle":"2022-12-24T11:03:17.709532Z","shell.execute_reply.started":"2022-12-24T11:03:17.251218Z","shell.execute_reply":"2022-12-24T11:03:17.708588Z"},"trusted":true},"execution_count":122,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  del sys.path[0]\n","output_type":"stream"},{"execution_count":122,"output_type":"execute_result","data":{"text/plain":"'[хорошая модель должна завтра да в да сто сорок ноль вас спасибо за за ожидание доброго до свидания]'"},"metadata":{}}]},{"cell_type":"code","source":"generate_text_nucleus('[номер телефона ')","metadata":{"execution":{"iopub.status.busy":"2022-12-24T11:05:36.395293Z","iopub.execute_input":"2022-12-24T11:05:36.397737Z","iopub.status.idle":"2022-12-24T11:05:36.803200Z","shell.execute_reply.started":"2022-12-24T11:05:36.397699Z","shell.execute_reply":"2022-12-24T11:05:36.801981Z"},"trusted":true},"execution_count":178,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  del sys.path[0]\n","output_type":"stream"},{"execution_count":178,"output_type":"execute_result","data":{"text/plain":"'[номер телефона сто сто тридцать двадцать семь ноль двадцать пять двадцать два девять]'"},"metadata":{}}]},{"cell_type":"markdown","source":"As you can see, now we do not get nonsense words","metadata":{}}]}