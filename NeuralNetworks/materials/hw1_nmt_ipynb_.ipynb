{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPcfHT5C-mKZ"
      },
      "source": [
        "# HW1: seq2seq nmt\n",
        "\n",
        "**Homework Goals**\n",
        "\n",
        "1. Get familiar with text data preparation\n",
        "2. Learn to work with RNN\n",
        "3. Train the model to translate `en-->ru`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hUc8aEg8S2_"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from collections import Counter\n",
        "import unicodedata\n",
        "import re\n",
        "import string\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdM5tBYV8S3J"
      },
      "source": [
        "## Naive way of texts representation:\n",
        "\n",
        "0. Normalize spelling\n",
        "1. Filter out all special characters\n",
        "2. Split by spaces, do *naive tokenization*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSYbvPOW8S3L"
      },
      "outputs": [],
      "source": [
        "# Prepare data and look at it\n",
        "# In addition to the dictionary, we are also interested in a set of characters\n",
        "raw_alphabet = set()\n",
        "alphabet = set()\n",
        "def normalize(s):\n",
        "    return \"\".join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def preprocess(s):\n",
        "    raw_alphabet.update(s)\n",
        "    s = normalize(s.lower().strip())\n",
        "    s = re.sub(r\"[^a-zа-я?.,!]+\", \" \", s)\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    alphabet.update(s)\n",
        "    return s\n",
        "\n",
        "pairs = []\n",
        "with open('eng-rus.txt', 'r') as fin:\n",
        "    for line in tqdm(fin.readlines()):\n",
        "        pair = [preprocess(_) for _ in line.split('\\t')]\n",
        "        pairs.append(pair)\n",
        "        \n",
        "print(\"RAW alphabet {} symbols:\".format(len(raw_alphabet)), \n",
        "      \"\".join(sorted(raw_alphabet)))\n",
        "print(\"After preprocessing {} symbols: \".format(len(alphabet)), \n",
        "      \"\".join(sorted(alphabet)))\n",
        "print(\"There are {} pairs\".format(len(pairs)))\n",
        "print(pairs[10000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9ARXr6b8S3U"
      },
      "source": [
        "Each word will be assigned a number + we will need special tokens for the beginning and end of the sequence and for unknown words.\n",
        "`<SOS>, <EOS>, <UNK>`\n",
        "\n",
        "We have two languages, to work with each we need functions for translating from words to numbers and vice versa.\n",
        "\n",
        "It is proposed to implement these functions as dictionaries. Allocate the first 4 numbers for special tokens\n",
        "\n",
        "**(1 point)** Implement the dictionary building function, the function takes a list of strings (normalized sentences, can be splited by spaces) as input. Organize the dictionary in a reasonable way so that rare words can be thrown out if necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2epLOt_-8S3V"
      },
      "outputs": [],
      "source": [
        "COMMON_TOKENS = ['PAD', 'SOS', 'EOS', 'UNK']\n",
        "\n",
        "\n",
        "def build_vocabs(sents, max_size=1000):\n",
        "    <your code>\n",
        "    return tok2idx, idx2tok\n",
        "\n",
        "\n",
        "eng, rus = list(zip(*pairs))\n",
        "rus2idx, idx2rus = build_vocabs(rus, max_size=10000)\n",
        "eng2idx, idx2eng = build_vocabs(eng, max_size=5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xh5koecS8S3c"
      },
      "outputs": [],
      "source": [
        "def sentence2idx(s, tok2idx):\n",
        "    tokens = preprocess(s).split(' ')\n",
        "    unk = tok2idx['UNK']\n",
        "    return [tok2idx['SOS']] + [tok2idx.get(_, unk) for _ in tokens] + [tok2idx['EOS']]\n",
        "\n",
        "\n",
        "def idx2sentence(s, idx2tok):\n",
        "    return \" \".join(idx2tok[_] for _ in s)\n",
        "\n",
        "# check the consistency of the transformations\n",
        "x = sentence2idx('Привет мир!', rus2idx)\n",
        "print(x)\n",
        "print(idx2sentence(x, idx2rus))\n",
        "\n",
        "x = sentence2idx('Hello world!', eng2idx)\n",
        "print(x)\n",
        "print(idx2sentence(x, idx2eng))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dhjjx52i8S3g"
      },
      "source": [
        "## Dealing with arbitrary length sequences in pytorch\n",
        "\n",
        "We need to be able to generate batches of `[bs, 1, seq_len]` tensors.\n",
        "But in our dataset, the samples are of different lengths:\n",
        "\n",
        "- we could cut everything down to the minimum length\n",
        "- padd to maximum length\n",
        "- choose some average length\n",
        "\n",
        "**(1 point)** Split the dataset on train and validate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmGX7wtL8S3i"
      },
      "outputs": [],
      "source": [
        "# make a dataset with encoded pairs:\n",
        "class EngRusDataset(Dataset):\n",
        "    def __init__(self, pairs):\n",
        "        self.pairs = pairs\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "    \n",
        "    def __getitem__(self, item):\n",
        "        eng, rus = self.pairs[item]\n",
        "        return dict(\n",
        "            eng=eng,\n",
        "            rus=rus,\n",
        "        )\n",
        "\n",
        "encoded = []\n",
        "for eng, rus in tqdm(pairs):\n",
        "    a = sentence2idx(eng, eng2idx)\n",
        "    b = sentence2idx(rus, rus2idx)\n",
        "    encoded.append((a, b))\n",
        "\n",
        "    \n",
        "<your code>\n",
        "trainset = EngRusDataset(...)\n",
        "valset = EngRusDataset(...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PDpAtEq8S3n"
      },
      "source": [
        "Let's build a naive DataLoader and check how it makes batches:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-wLIzt88S3o"
      },
      "outputs": [],
      "source": [
        "trainloader = DataLoader(trainset, batch_size=8, shuffle=True)\n",
        "it = iter(trainloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CM0vzGL8S3s"
      },
      "outputs": [],
      "source": [
        "batch = next(it)['eng']\n",
        "batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7blZtcq8S3w"
      },
      "source": [
        "In my case, the result was:\n",
        "```\n",
        "[tensor([1, 1, 1, 1, 1, 1, 1, 1]),\n",
        " tensor([ 6,  7,  6, 15,  5,  6,  5, 62]),\n",
        " tensor([ 48,  34,  83,   7,  32, 221,  22,  43]),\n",
        " tensor([  5, 143,  37,  36, 129,  12,  11,  66]),\n",
        " tensor([  73, 1258,  279,    8,    6,  555,   41,   10]),\n",
        " tensor([  8, 140,   8, 628,  20,  96,  13, 270]),\n",
        " tensor([  47,    4,   15,   18,   55,  269,    6, 1287]),\n",
        " tensor([ 58,   2,  13, 140, 193, 140, 171, 140])]\n",
        "```\n",
        "\n",
        "What's weird here?\n",
        "1. This is not a tensor, but a list of tensors. Accordingly, when iterating over zero dimension (`batch[i, :]`), we will get not an i-example, but i-tokens for all examples in the batch. This is not a problem, but different from the expected behavior.\n",
        "2. Only one example ends with `<EOS>` (2), the others are cut off to match its length. And this is a problem.\n",
        "\n",
        "We would like to padd all examples to the maximum length in the batch.\n",
        "But at the stage of preparing the example (in the `__getitem__` function), we do not know the batch neighbors!\n",
        "In order to change the batch merging logic, we need to write our own `collate_fn` function in the DataLoader constructor:\n",
        "\n",
        "```\n",
        "def collate_fn(samples):\n",
        "    # samples -- list of dictionaries samples\n",
        "    <...>\n",
        "    return batch\n",
        "```\n",
        "\n",
        "**(1 point)** Write a `collate_fn` function that padds _correctly_ rus and eng sequences and merges them into batches, where `batch[i, :]` returns the tokens for the `i` example.\n",
        "\n",
        "Expected output (for a sequence with left padding):\n",
        "\n",
        "```\n",
        "tensor([[   1,   10, 3429,  405,  113,  676,   10, 1031,  140,    4,    2],\n",
        "        [   0,    1,   57,   18,   23,   19,   61,    7,  140,    4,    2],\n",
        "        [   0,    0,    0,    1,   16,   17, 1131,  416,  140,    4,    2],\n",
        "        [   0,    0,    0,    1,   13,  465,   75,  197,  140,    4,    2],\n",
        "        [   0,    0,    0,    1,    6,  302,   13,  144,  140,    4,    2],\n",
        "        [   0,    1,    6,   59,  205,  167,    8,   15,  140,    4,    2],\n",
        "        [   0,    0,    0,    0,    1,    6,   14,  678,  140,    4,    2],\n",
        "        [   0,    0,    1,    5,   29,   67,    6,   14,  140,    4,    2]])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMSeFFHQ8S3y"
      },
      "outputs": [],
      "source": [
        "def collate_fn(samples):\n",
        "    # <your code>\n",
        "    return dict(\n",
        "        rus=...,\n",
        "        eng=...,\n",
        "    )\n",
        "    \n",
        "\n",
        "trainloader = DataLoader(trainset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
        "it = iter(trainloader)\n",
        "next(it)['eng']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDcGAXiA-mKm"
      },
      "source": [
        "Now we have the correct data generator, and all we have to do is write the model (encoder and decoder).\n",
        "\n",
        "\n",
        "### Encoder\n",
        "\n",
        "The input tensor contains integers and has dimensions `[bs, seq_len]`,\n",
        "\n",
        "We will pass them through the layer with embeddings and get the tensor `[bs, seq_len, dim]`. Now we have floating point numbers that can be fed to RNN layers as input.\n",
        "\n",
        "\n",
        "\n",
        "GRU is an RNN with a specific structure:\n",
        "<img src=\"https://habrastorage.org/webt/xt/_q/nj/xt_qnjgfjengqoqd4gizkq4j_wk.png\">\n",
        "\n",
        "In the picture, the yellow rectangles are the line layers with the corresponding activation functions.\n",
        "\n",
        "\n",
        "`nn.RNN` allows you to create and use multi-layer one- and two-way layers as one layer.\n",
        "All parameters must be specified during creation, and then simply applied during the forward pass.\n",
        "\n",
        "\n",
        "The order of dimensions is a bit different from the usual in convolutional networks, this is due to the inability to parallel recurrent calculations effectively.\n",
        "\n",
        "\n",
        "**batch_first=True**\n",
        "\n",
        "Such an RNN layer expects two tensors as input:\n",
        "  - input with sizes `[bs, seq_len, dim]`,\n",
        "  - hidden_state with dimensions `[num_layers * num_directions, bs, hidden_size]`.\n",
        " \n",
        " \n",
        "The output is two tensors:\n",
        "- output `[bs, seq_len, dim]`,\n",
        "- hidden `[num_layers * num_directions, bs, hidden]`.\n",
        "\n",
        "We will apply RNN in two ways:\n",
        "- to the entire sequence, to translate the entire phrase in one language into one vector (EncoderRNN)\n",
        "- to one tensor and input token to generate a phrase in another language (DecoderRNN)\n",
        "\n",
        "\n",
        "We will put the entire input sequence into a hidden state vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dph7rI9_8S33"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, vocab_size, layers=1):\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "        \n",
        "        self.embeddings = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True, num_layers=layers)\n",
        "        \n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embeddings(input)\n",
        "        output, hidden = self.rnn(embedded, hidden)\n",
        "        return output, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size=1, device=None):\n",
        "        # be aware about dimension! https://pytorch.org/docs/stable/nn.html#torch.nn.GRU\n",
        "        return torch.zeros(self.layers, batch_size, self.hidden_size, device=device)\n",
        "\n",
        "\n",
        "enc = EncoderRNN(256, len(eng2idx))\n",
        "x = next(it)['eng']\n",
        "print(x.shape)\n",
        "hidden = enc.init_hidden(8)\n",
        "out, hidden = enc(x, hidden)\n",
        "print(out.shape, hidden.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vg6cvr8L-mKm"
      },
      "source": [
        "We want the decoder to generate a translation for us -- a sequence of tokens from another language, using the encoder's hidden state vector.\n",
        "\n",
        "To do this, we will supply hidden and `<SOS>`token to the input.\n",
        "At each step, the decoder will return hidden and output vector.\n",
        "Output vector is the probability distribution for the next token (respectively, it has the size of the output language dictionary)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYfecQb88S38"
      },
      "outputs": [],
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, vocab_size, layers=1):\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "        \n",
        "        self.embeddings = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True, num_layers=layers)\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=2)\n",
        "        \n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embeddings(input)\n",
        "        output, hidden = self.rnn(embedded, hidden)\n",
        "        output = self.softmax(self.out(output))\n",
        "        return output, hidden\n",
        "        \n",
        "    def init_hidden(self, batch_size=1, device=None):\n",
        "        return torch.zeros(self.layers, batch_size, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyMwF94n8S3_"
      },
      "outputs": [],
      "source": [
        "dec = DecoderRNN(256, len(rus2idx))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_Vj3_PB-mKn"
      },
      "source": [
        "Let's get a tensor with tokens of size `[bs, seq_len]` from the data generator and try to iterate over seq_len to generate the next token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVFcg0Uz8S4C"
      },
      "outputs": [],
      "source": [
        "batch = next(it)['rus'] # get batch\n",
        "bs, seq_len = batch.shape\n",
        "for i in range(0, seq_len):\n",
        "    step = y[:, i].unsqueeze(1)  # get tokens sample for i-th step \n",
        "     # These are the correct tokens (ground truth), we could generate them\n",
        "     # unsqueeze adds dimension 1 (from [bs] to [bs, 1])\n",
        "    output, hidden = dec(t, hidden)\n",
        "    print(output.shape, hidden.shape)\n",
        "    # output -- this is the probability distribution for the next token\n",
        "    # hidden -- this is the updated hidden state"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(6 points)** Fill in a training part and train the encoder and decoder.\n",
        "\n",
        "1. You need to write getting the next token (integer) from the distribution: a vector of size `len(rus2idx)`. Since we are working in batches, this should be a batchified operation. You have several options for how to do this:\n",
        " - take by argmax\n",
        " - sample from distribution (torch.multinomial)\n",
        " - during training, take tokens from ground truth (and this must be done at least sometimes so that the model converges).\n",
        " \n",
        "2. You need to write a loss calculation. It is convenient to do this at each step: after the `<EOS>` occurs in the example, you do not need to count the loss for it (in the vectorized version, you can multiply the loss for `<PAD>`-tokens by zero - this is called masking). Loss is simply the sum of cross-entropy losses for each step.\n"
      ],
      "metadata": {
        "id": "ECBleoB4F58E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EshFSblS-mKn"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "def train(model, optimizer, dataloader): \n",
        "    encoder, decoder = model\n",
        "    encoder.to(device)\n",
        "    decoder.to(device)\n",
        "    \n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    logs = defaultdict(list)\n",
        "    for batch in tqdm(dataloader):\n",
        "        rus = batch['rus'].to(device)\n",
        "        eng = batch['eng'].to(device)\n",
        "        encoder_hidden = encoder.init_hidden(eng.size(0)).to(device)\n",
        "        encoder_outputs, hidden = encoder(eng, encoder_hidden)\n",
        "        \n",
        "        # write decoder application and loss calculation.\n",
        "        # hint: loss must be masked, in case the sequence has already ended.\n",
        "        <your code>\n",
        "                \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        logs['loss'].append(loss.item())\n",
        "    return logs\n",
        "\n",
        "def validate(model, dataloader):\n",
        "    logs = defaultdict(list)\n",
        "    for batch in tqdm(dataloader):\n",
        "        <your code>\n",
        "        logs['loss'].append(loss.item())\n",
        "    \n",
        "    return {k: [np.mean(v)] for k, v in logs.items()}\n",
        "\n",
        "def plot_logs(logs):\n",
        "    clear_output()\n",
        "    plt.figure()\n",
        "    plt.plot(logs['loss'], zorder=1)\n",
        "    plt.scatter(logs['steps'], logs['val_loss'], marker='+', s=180, c='orange', label='val', zorder=2)\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure()\n",
        "    # use label&legend to display labels\n",
        "    # plt.plot(..., label=name)\n",
        "    # plt.legend() \n",
        "    <your code>        \n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThQj4uOx8S4P"
      },
      "outputs": [],
      "source": [
        "encoder = EncoderRNN(256, len(eng2idx)).to(device)\n",
        "decoder = DecoderRNN(256, len(rus2idx)).to(device)\n",
        "opt = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=1e-2)\n",
        "trainloader = DataLoader(trainset, batch_size=128, shuffle=True, collate_fn=collate_fn)\n",
        "valloader = DataLoader(valset, batch_size=128, shuffle=False, collate_fn=collate_fn)\n",
        "model = (encoder, decoder)\n",
        "\n",
        "\n",
        "train_model(model, opt, trainloader, valloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(2 points)** Write a translation function with sampling from a distribution with temperature."
      ],
      "metadata": {
        "id": "R8G9EoojGDdJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64h3_w528S4d"
      },
      "outputs": [],
      "source": [
        "encoder.eval()\n",
        "decoder.eval()\n",
        "encoder = encoder.to(\"cpu\")\n",
        "decoder = decoder.to(\"cpu\")\n",
        "\n",
        "def evaluate(sentence, T=1.0):\n",
        "    encoded = sentence2idx(sentence, eng2idx)\n",
        "    output = []\n",
        "    print(encoded)\n",
        "    bs = 10\n",
        "    with torch.no_grad():\n",
        "      \n",
        "        z = torch.LongTensor(encoded).view(1, -1).repeat(bs, 1)\n",
        "        encoder_outputs, hidden = encoder(z, encoder.init_hidden(bs))\n",
        "        \n",
        "        for i in range(20):\n",
        "            <your code>\n",
        "            output.append(tokens)\n",
        "    \n",
        "    output = np.array(output).T\n",
        "    for s in output:\n",
        "        out = idx2sentence(s, idx2rus)\n",
        "        print(out.replace('PAD', \"\"))\n",
        "\n",
        "    \n",
        "evaluate(\"What is going on?\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "base"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}