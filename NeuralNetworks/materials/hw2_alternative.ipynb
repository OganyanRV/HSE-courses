{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Igxuy75W5V6j"
      },
      "source": [
        "# Transformers\n",
        "\n",
        "In this homework, you need to generate text with a neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "N8_Ha3x-5V6m"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import numpy as np\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFXsDKRZ5V6n"
      },
      "source": [
        "## Task 1 (0.5 points)\n",
        "We will train a language model to predict the next letter. Such language models are used in speech recognition, as they provide additional information to the acoustic model when the next character is selected. To get started, open the data, check what characters are included in the texts, how many there are. Remove all newline characters and tabs from the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "1M_I40Zl5V6n"
      },
      "outputs": [],
      "source": [
        "path = 'small_corp_for_test.txt'\n",
        "file = open(path, 'r')\n",
        "data = file.readlines()\n",
        "file.close()\n",
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDpR5MxM5V6o"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlGTWEMK5V6o"
      },
      "source": [
        "## Task 2 (0.5 points)\n",
        "To train the model, you have to change the text to a form suitable for the neural network. It is also important to note that we need to add two tokens (start and end), which are responsible for the beginning and end of the text. Use [ and ] for this task. We also need a pad token to fill the text with it to the required length to form a batch.\n",
        "\n",
        "Implement the preprocess method of the Preprocessor class. As input it takes text and the length of text that we expect to receive as output. The text must be converted to lower case, the required number of pad tokens are added to the end of the text, then the text is vectorized (each character has its own number). You need to return two vectors: the result obtained without the last token (we will train on it) and the result obtained without the first token (target during training)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCu24Nez5V6o"
      },
      "outputs": [],
      "source": [
        "class Preprocessor:\n",
        "    def __init__(self):\n",
        "        self.alphabet = '_добсркгаупитнезчм яжлйвцыэь-шхющёъ][ '\n",
        "        self.token2ind = {}\n",
        "        self.ind2token = {}\n",
        "        for i in range(len(self.alphabet)):\n",
        "            self.token2ind[self.alphabet[i]] = i\n",
        "            self.ind2token[i] = self.alphabet[i]\n",
        "        \n",
        "    \n",
        "    def preprocess(self, text, window_size):\n",
        "        # YOUR CODE HERE\n",
        "        pass\n",
        "        #################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRqJTtxC5V6p"
      },
      "source": [
        "## Task 3 (0.5 points)\n",
        "Since we decided that the text will begin with the token [ and end with the token ], the data needs to be corrected. Implement this idea, add these tokens to your texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "417DsdjZ5V6p"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tl_QKCTW5V6p"
      },
      "source": [
        "## Task 4 (0.5 points)\n",
        "Let's limit the maximum text length. You can change this threshold and thereby reduce the number of texts in your sample and increase the learning rate. Let's start with 128.\n",
        "Select a threshold and leave only those texts whose length does not exceed this threshold.\n",
        "\n",
        "Next, split the texts into train and test, mix the texts when splitting, the size of the test sample should be 15% of the total number of texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ue53yUA65V6q"
      },
      "outputs": [],
      "source": [
        "THRESHOLD = 128\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRRp7W1j5V6q"
      },
      "source": [
        "## Task 5 (1.5 points)\n",
        "Let's write a dataset. The input to the dataset is a set of texts, an object of the Preprocessor class, and the window size that you selected in the previous task.\n",
        "Implement the __len__ and __getitem__ methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCA9-2NP5V6q"
      },
      "outputs": [],
      "source": [
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    \n",
        "    def __init__(self, x, preproc, win_size = 128):\n",
        "        # YOUR CODE HERE\n",
        "        pass\n",
        "        ################\n",
        "    \n",
        "    def __len__(self):\n",
        "        # YOUR CODE HERE\n",
        "        pass\n",
        "        ################\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # YOUR CODE HERE\n",
        "        pass\n",
        "        ################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZhUrshc5V6q"
      },
      "outputs": [],
      "source": [
        "preproc = Preprocessor()\n",
        "train_dataset = TextDataset(data_train, preproc)\n",
        "test_dataset = TextDataset(data_test, preproc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4GIeH3n5V6q"
      },
      "source": [
        "\n",
        "\n",
        "## Task 6 (1.5 points)\n",
        "Let's write a model. The class for implementing positional encoding is implemented for you, it is needed so that the model (after receiving embeddings) can understand, in which place which token is located.\n",
        "\n",
        "Fill in the blanks in the model class. Choose the hyperparameters of the model. It is recommended to use no more than 6 layers in the transformer. In the decoder, use two linear layers with a ReLU activation function in between."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIbyJo_u5V6r"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80WG1r7O5V6r"
      },
      "outputs": [],
      "source": [
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(LanguageModel, self).__init__()\n",
        "        self.emb = ...\n",
        "        self.pe = ...\n",
        "        self.transformer_encoder_layer = ...\n",
        "        self.transformer_encoder = ...\n",
        "        self.decoder = ...\n",
        "    \n",
        "    def forward(self, x, src_mask):\n",
        "        x = ... # emb, then pe\n",
        "        x = x.transpose(1, 0)\n",
        "        x = ... # transformer encoder with mask\n",
        "        x = ... # decoder\n",
        "        return x.transpose(1, 0)\n",
        "    \n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        \n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlTy35Pu5V6r"
      },
      "outputs": [],
      "source": [
        "model = LanguageModel(len('_добсркгаупитнезчм яжлйвцыэь-шхющёъ][ '))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYOtJnqB5V6r"
      },
      "source": [
        "## Task 7 (2 points)\n",
        "Let's implement a class to train the model and validate it. Follow the directions in the code and fill in the missing pieces in the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fXoWIt75V6r"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    \n",
        "    def __init__(self, model, train_dataset, test_dataset):\n",
        "        \n",
        "        self.model = model\n",
        "        \n",
        "        self.train_batch_size = 64\n",
        "        self.test_batch_size = 64\n",
        "        \n",
        "        self.train_dataloader = ...\n",
        "        self.test_dataloader = ...\n",
        "        self.train_dataloader_size = ...\n",
        "        self.test_dataloader_size = ...\n",
        "        \n",
        "        self.device = 'cuda:0'\n",
        "        self.criterion = ... # use CrossEntrophyLoss, pass as a parameter\n",
        "                             # ignore the index of the _ character so that the model is not penalized for the character after the closing token\n",
        "\n",
        "        \n",
        "        self.optimizer = ...\n",
        "        \n",
        "        self.steps_to_print = 1000\n",
        "        \n",
        "    def train_one_epoch(self, epoch_number):\n",
        "        step = 0\n",
        "        counted_loss = 0\n",
        "        current_time = time.time()\n",
        "        it = 0\n",
        "        \n",
        "        for batch in self.train_dataloader:\n",
        "            x, y = batch\n",
        "            # YOUR CODE HERE\n",
        "            \n",
        "            # implement model training steps\n",
        "\n",
        "            # store the loss value in counted_loss variable\n",
        "\n",
        "            ################\n",
        "            \n",
        "            \n",
        "            if step%self.steps_to_print == 0:\n",
        "                result = 'Train epoch '+str(epoch_number)+' | '\n",
        "                result += 'Step '+str(step)+'/'+str(self.train_dataloader_size)+' | '\n",
        "                result += 'Counted loss '+str(counted_loss)+' | '\n",
        "                result += 'ppl '+str(math.exp(counted_loss/it))+' | '\n",
        "                result += 'time '+str(time.time() - current_time) + ' | '\n",
        "                print(result)\n",
        "                current_time = time.time()\n",
        "                counted_loss = 0\n",
        "                it = 0\n",
        "    \n",
        "    def validate_one_epoch(self, epoch_number):\n",
        "        step = 0\n",
        "        counted_loss = 0\n",
        "        current_time = time.time()\n",
        "        it = 0\n",
        "        for batch in self.test_dataloader:\n",
        "            x, y = batch\n",
        "            \n",
        "            # YOUR CODE HERE\n",
        "\n",
        "\n",
        "            # implement steps for test\n",
        "            # remember that this method is already run from the block with torch.no_grad(), so you don't need to reuse it\n",
        "            \n",
        "            ################\n",
        "            \n",
        "            if step%(self.steps_to_print//2) == 0:\n",
        "                result = 'Validate epoch '+str(epoch_number)+' | '\n",
        "                result += 'Step '+str(step)+'/'+str(self.test_dataloader_size)+' | '\n",
        "                result += 'Counted loss '+str(counted_loss)+' | '\n",
        "                result += 'ppl '+str(math.exp(counted_loss/it))+' | '\n",
        "                result += 'time '+str(time.time() - current_time) + ' | '\n",
        "                print(result)\n",
        "                current_time = time.time()\n",
        "                counted_loss = 0\n",
        "                it = 0\n",
        "        \n",
        "    def train(self, number_of_epochs):\n",
        "        model.to(self.device)\n",
        "        for epoch in range(1, number_of_epochs+1):\n",
        "            model.train()\n",
        "            self.train_one_epoch(epoch)\n",
        "            with torch.no_grad():\n",
        "                model.eval()\n",
        "                self.validate_one_epoch(epoch)\n",
        "            print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGt0oTKX5V6s"
      },
      "source": [
        "## Task 8 (0.5 points)\n",
        "Run training on multiple epochs. Focus on your computing power and work time. You can always calculate how many seconds it takes for one batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzbpIeNi5V6s"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "###############"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XU8w0PP5V6s"
      },
      "source": [
        "## Task 9 (1 point)\n",
        "Let's try to generate text with our model. Finish the text generation function. Try to generate some text. Remember that if you want to generate text from scratch, you must pass only the start token as text.\n",
        "Stop generating text if the model gives you an end token or if the text length is greater than 150."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rZHjQNB5V6s"
      },
      "outputs": [],
      "source": [
        "def generate_text(text):\n",
        "    x = []\n",
        "    \n",
        "    for letter in text:\n",
        "        x.append(preproc.token2ind[letter])\n",
        "    x = torch.from_numpy(np.array(x))\n",
        "    \n",
        "    pred = ...\n",
        "    ind = ... \n",
        "    \n",
        "    text += ... \n",
        "    \n",
        "    if ...:\n",
        "        return text\n",
        "    else:\n",
        "        return generate_text(text)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "a55375f1da33de460d61f26d01bd36bfbcd8e12064599fa5a58833b6ccc82f6a"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}